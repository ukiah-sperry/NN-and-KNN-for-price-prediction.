import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
from torch.utils.data import Dataset, DataLoader

df = pd.read_csv("auto.csv")

# Binary encoding for columns with 2 unique options
binary_cols = [col for col in df.columns if df[col].nunique() == 2]
for col in binary_cols:
    unique_values = df[col].unique()
    df[col] = df[col].apply(lambda x: 1 if x == unique_values[0] else 0)

# One-Hot Encoding for remaining categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Normalize numeric features
scaler = StandardScaler()
numeric_cols = ['km', 'age', 'hp_kW', 'Displacement_cc', 'Weight_kg', 'cons_comb']
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Apply log transformation to the target
df['price'] = np.log1p(df['price'])

# Define features (X) and target (y)
X = df.drop('price', axis=1).values
y = df['price'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert data to PyTorch tensors
X_train = X_train.astype(np.float32)
X_test = X_test.astype(np.float32)

X_train_tensor = torch.tensor(X_train)
X_test_tensor = torch.tensor(X_test)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

# Create a custom PyTorch Dataset
class AutoDataset(Dataset):
    def __init__(self, features, targets):
        self.features = features
        self.targets = targets

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, index):
        return self.features[index], self.targets[index]

# Create PyTorch Datasets
train_dataset = AutoDataset(X_train_tensor, y_train_tensor)
test_dataset = AutoDataset(X_test_tensor, y_test_tensor)

batch_size = 32
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

print(f"Number of training samples: {len(train_dataset)}")
print(f"Number of testing samples: {len(test_dataset)}")



numeric_cols = ['km', 'age', 'hp_kW', 'Displacement_cc', 'Weight_kg', 'cons_comb']

scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

scaled_stats = df[numeric_cols].describe()
print(scaled_stats)


import matplotlib.pyplot as plt
import seaborn as sns

skewness = df['price'].skew()
print(f'Skewness of price: {skewness:.2f}')
# Plot price distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['price'], bins=50, kde=True, color='blue')
plt.title('Price Distribution')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()





numerical_cols = ['km', 'age', 'hp_kW', 'Displacement_cc', 'Weight_kg', 'cons_comb', 'price']
correlation_matrix = df[numerical_cols].corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()



categorical_cols_subset = ['make_model', 'Previous_Owners', 'body_type']
raw_df = pd.read_csv("auto.csv")
df_visual = raw_df.copy()  # Correctly copy raw_df into df_visual

for col in categorical_cols_subset:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df_visual[col], y=df_visual['price'])
    plt.xticks(rotation=45)
    plt.title(f"Price vs {col}")
    plt.xlabel(col)
    plt.ylabel("Price")
    plt.tight_layout()
    plt.show()


# Filter top 10 highest and lowest-priced Audi A3s
top_10_audi = raw_df[raw_df['make_model'] == 'Audi A3'].nlargest(10, 'price')
bottom_10_audi = raw_df[raw_df['make_model'] == 'Audi A3'].nsmallest(10, 'price')

# Categories to analyze
categories = ['Comfort_Convenience', 'Entertainment_Media', 'Extras']

def count_features(df, category):
    feature_counts = {}
    for features in df[category]:
        if pd.notna(features):
            for feature in features.split(','):
                feature = feature.strip()
                feature_counts[feature] = feature_counts.get(feature, 0) + 1
    return feature_counts

for category in categories:
    top_counts = count_features(top_10_audi, category)
    bottom_counts = count_features(bottom_10_audi, category)

    feature_data = pd.DataFrame({
        'Feature': list(top_counts.keys()),
        'Top 10 Count': list(top_counts.values()),
        'Bottom 10 Count': [bottom_counts.get(feature, 0) for feature in top_counts.keys()]
    }).sort_values(by='Top 10 Count', ascending=False)

    plt.figure(figsize=(12, 6))
    sns.barplot(data=feature_data, x='Feature', y='Top 10 Count', label='Top 10 Prices', color='blue', alpha=0.6)
    sns.barplot(data=feature_data, x='Feature', y='Bottom 10 Count', label='Bottom 10 Prices', color='red', alpha=0.6)
    plt.xticks(rotation=45, ha='right')
    plt.title(f'Feature Counts in {category} for Top vs Bottom Audi A3 Prices')
    plt.xlabel('Features')
    plt.ylabel('Count')
    plt.legend()
    plt.tight_layout()
    plt.show()






for col in numerical_cols[:-1]: 
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=raw_df[col])  # Use raw_df here
    plt.title(f"Outlier Detection for {col}")
    plt.xlabel(col)
    plt.show()





top_20_km_raw = raw_df.nlargest(20, 'km')[['km']]
print(top_20_km_raw)


plt.figure(figsize=(8, 4))
plt.hist(raw_df['km'], bins=30, edgecolor='k')  # Use the raw 'km' column from raw_df
plt.title("Distribution of Actual Kilometers")
plt.xlabel("Kilometers")
plt.ylabel("Frequency")
plt.show()





import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import r2_score

class AutoNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(AutoNN, self).__init__()
        self.l1 = nn.Linear(input_size, hidden_size)
        self.relu1 = nn.ReLU()
        self.l2 = nn.Linear(hidden_size, hidden_size // 2)
        self.relu2 = nn.ReLU()
        self.output = nn.Linear(hidden_size // 2, 1)  # Output layer for regression

    def forward(self, x):
        x = self.l1(x)
        x = self.relu1(x)
        x = self.l2(x)
        x = self.relu2(x)
        x = self.output(x)
        return x

# Model parameters
input_size = X_train.shape[1]
hidden_size = 128
num_epochs = 200
learning_rate = 0.01

# Initialize the model, loss function, and optimizer
model = AutoNN(input_size, hidden_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)



for epoch in range(num_epochs):
    model.train()
    for features, targets in train_loader:
        # Forward pass
        outputs = model(features)
        loss = criterion(outputs, targets)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

model.eval()
with torch.no_grad():
    y_pred = model(X_test_tensor).view(-1, 1)
    mse = criterion(y_pred, y_test_tensor).item()
    r2 = r2_score(y_test_tensor.numpy(), y_pred.numpy())
    mape = torch.mean(torch.abs((y_test_tensor - y_pred) / y_test_tensor)).item() * 100
    print(f"Mean Squared Error on Test Data: {mse:.4f}")
    print(f"R-squared (RÂ²) on Test Data: {r2:.4f}")
    print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")



